{"cells":[{"cell_type":"code","execution_count":null,"id":"NAKPxbVytVOl","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46040,"status":"ok","timestamp":1711875173140,"user":{"displayName":"Mahmoud Taha","userId":"01043581029630877833"},"user_tz":-120},"id":"NAKPxbVytVOl","outputId":"7cf99779-111e-4f3c-c83b-0cd2d0299457"},"outputs":[],"source":["## extract classes \n","\n","import os\n","import shutil\n","\n","def extract_classes(source, classes):\n","\n","    dirname = os.path.dirname(__file__)\n","    source_directory = os.path.join(dirname, source)\n","    print(\"source: \" + source_directory)\n","  \n","    for c in classes:\n","        temp = os.path.join(dirname, c)\n","        print( c + \": \" + temp)\n","        os.makedirs(temp, exist_ok=True)\n","\n","    for filename in os.listdir(source_directory):\n","        for c in classes:\n","            if c in filename:\n","                shutil.move(os.path.join(source_directory, filename), os.path.join(dirname,c, filename))\n","\n","\n","\n","extract_classes(\"raw\" , [\"cat\" , \"dog\"])"]},{"cell_type":"code","execution_count":null,"id":"RWYChiLbr06e","metadata":{"executionInfo":{"elapsed":479,"status":"ok","timestamp":1711877825803,"user":{"displayName":"Mahmoud Taha","userId":"01043581029630877833"},"user_tz":-120},"id":"RWYChiLbr06e"},"outputs":[],"source":["## split folders\n","# pip install split-folders\n","\n","import splitfolders\n","import os\n","\n","dirname = os.path.dirname(__file__)\n","# input: folder that contsins all classes sub folders\n","splitfolders.ratio(os.path.join(dirname, \"raw\"), output=os.path.join(dirname, \"processed\"), seed=42, ratio=(0.6, 0.2, 0.2))\n"]},{"cell_type":"code","execution_count":16,"id":"6d9cf625","metadata":{"executionInfo":{"elapsed":35915,"status":"ok","timestamp":1711877867153,"user":{"displayName":"Mahmoud Taha","userId":"01043581029630877833"},"user_tz":-120},"id":"6d9cf625"},"outputs":[{"name":"stdout","output_type":"stream","text":["folder: D:\\Artificial Intelligence\\Machine Learning\\_projects\\dogs_cats_classification\\data\\processed\\train\n","15000\n","<class 'list'>\n","[[[ 87 164 203]\n","  [ 93 170 209]\n","  [ 94 171 210]\n","  ...\n","  [125 207 247]\n","  [119 204 245]\n","  [122 201 240]]\n","\n"," [[ 87 164 203]\n","  [ 93 170 209]\n","  [ 94 171 210]\n","  ...\n","  [128 209 247]\n","  [122 205 245]\n","  [123 202 241]]\n","\n"," [[ 87 164 203]\n","  [ 93 170 209]\n","  [ 94 171 210]\n","  ...\n","  [130 210 247]\n","  [125 206 244]\n","  [124 203 242]]\n","\n"," ...\n","\n"," [[ 53 124 158]\n","  [ 55 126 160]\n","  [ 56 127 161]\n","  ...\n","  [  0   4   3]\n","  [  0   4   3]\n","  [  0   2   2]]\n","\n"," [[ 56 123 154]\n","  [ 58 125 156]\n","  [ 60 127 158]\n","  ...\n","  [  1   3   3]\n","  [  1   3   3]\n","  [  1   3   3]]\n","\n"," [[ 54 121 152]\n","  [ 55 122 153]\n","  [ 59 126 157]\n","  ...\n","  [  0   2   2]\n","  [  0   2   2]\n","  [  0   2   2]]]\n","folder: D:\\Artificial Intelligence\\Machine Learning\\_projects\\dogs_cats_classification\\data\\processed\\test\n","5000\n","<class 'list'>\n","[[[ 68 120 144]\n","  [ 62 114 137]\n","  [ 64 116 139]\n","  ...\n","  [179 155 209]\n","  [188 164 218]\n","  [177 159 208]]\n","\n"," [[ 63 115 138]\n","  [ 62 114 137]\n","  [ 59 112 134]\n","  ...\n","  [187 163 217]\n","  [181 157 211]\n","  [185 163 212]]\n","\n"," [[ 62 114 137]\n","  [ 65 117 140]\n","  [ 65 117 140]\n","  ...\n","  [182 158 211]\n","  [184 160 214]\n","  [186 160 210]]\n","\n"," ...\n","\n"," [[ 19  17  22]\n","  [ 23  21  27]\n","  [ 20  19  24]\n","  ...\n","  [164 144 190]\n","  [157 145 194]\n","  [157 141 195]]\n","\n"," [[ 17  17  19]\n","  [ 18  16  18]\n","  [ 19  18  20]\n","  ...\n","  [155 166 184]\n","  [156 149 192]\n","  [162 143 198]]\n","\n"," [[ 20  19  21]\n","  [ 22  21  23]\n","  [ 19  18  20]\n","  ...\n","  [162 173 191]\n","  [159 151 195]\n","  [162 144 197]]]\n","folder: D:\\Artificial Intelligence\\Machine Learning\\_projects\\dogs_cats_classification\\data\\processed\\val\n","5000\n","<class 'list'>\n","[[[ 41  44  40]\n","  [ 44  45  41]\n","  [ 49  44  41]\n","  ...\n","  [155 166 165]\n","  [180 205 208]\n","  [163 196 200]]\n","\n"," [[ 39  44  39]\n","  [ 41  42  38]\n","  [ 45  41  37]\n","  ...\n","  [150 160 160]\n","  [169 191 196]\n","  [166 196 202]]\n","\n"," [[ 38  42  37]\n","  [ 36  37  33]\n","  [ 40  36  32]\n","  ...\n","  [153 159 161]\n","  [164 183 190]\n","  [166 194 201]]\n","\n"," ...\n","\n"," [[ 21  20  22]\n","  [ 18  17  19]\n","  [ 18  17  19]\n","  ...\n","  [ 35  39  55]\n","  [ 65  71  84]\n","  [ 19  25  36]]\n","\n"," [[ 27  26  28]\n","  [ 21  20  22]\n","  [ 21  20  22]\n","  ...\n","  [ 30  35  51]\n","  [ 28  34  47]\n","  [ 25  31  43]]\n","\n"," [[ 29  28  30]\n","  [ 20  19  22]\n","  [ 20  19  21]\n","  ...\n","  [ 41  47  63]\n","  [ 29  37  50]\n","  [ 26  35  45]]]\n"]}],"source":["## read and resized images\n","# pip install opencv-python\n","\n","import glob\n","import cv2\n","import numpy as np\n","import os\n","\n","target_size = (128, 128)\n","\n","def read_resize_images(source_dir , categories):\n","    x_temp = []\n","    y_temp = []\n","    for category in categories:\n","        for filepath in glob.glob(os.path.join(source_dir, category, \"*\")):\n","            label = 0 if category == \"cats\" else 1\n","            img = cv2.imread(filepath)\n","            img = cv2.resize(img, target_size)\n","            img = np.array(img)\n","            y_temp.append(label)\n","            x_temp.append(img)\n","\n","    print(\"folder: \" + os.path.join(source_dir))\n","    print(len(x_temp))\n","    print(type(x_train)) \n","    print(x_temp[0])   \n","    return x_temp , y_temp\n","\n","# source file, catefories list\n","x_train , y_train = read_resize_images( \"D:\\\\Artificial Intelligence\\\\Machine Learning\\\\_projects\\\\dogs_cats_classification\\\\data\\\\processed\\\\train\", [\"cat\", \"dog\"])\n","x_test , y_test = read_resize_images( \"D:\\\\Artificial Intelligence\\\\Machine Learning\\\\_projects\\\\dogs_cats_classification\\\\data\\\\processed\\\\test\", [\"cat\", \"dog\"])\n","x_val , y_val = read_resize_images( \"D:\\\\Artificial Intelligence\\\\Machine Learning\\\\_projects\\\\dogs_cats_classification\\\\data\\\\processed\\\\val\", [\"cat\", \"dog\"])\n","\n"]},{"cell_type":"code","execution_count":17,"id":"c0c5f121","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'numpy.ndarray'>\n"]}],"source":["print(type(x_train[0]))"]},{"cell_type":"code","execution_count":23,"id":"VLoDTv1DwRGf","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":395,"status":"ok","timestamp":1711877907295,"user":{"displayName":"Mahmoud Taha","userId":"01043581029630877833"},"user_tz":-120},"id":"VLoDTv1DwRGf","outputId":"6e4ef105-bbcc-449f-a2ba-ca639c4021a0"},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"sequential_4\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63504</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,064,320</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │           \u001b[38;5;34m448\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_9 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63504\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │     \u001b[38;5;34m4,064,320\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,064,833</span> (15.51 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,064,833\u001b[0m (15.51 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,064,833</span> (15.51 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,064,833\u001b[0m (15.51 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"source":["## build model\n","# pip install tensorflow\n","# pip install keras\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","\n","# Define the model\n","model = Sequential([\n","    Conv2D(16, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n","    MaxPooling2D((2, 2)),\n","    # Conv2D(64, (3, 3), activation='relu'),\n","    # MaxPooling2D((2, 2)),\n","    # Conv2D(128, (3, 3), activation='relu'),\n","    # MaxPooling2D((2, 2)),\n","    Flatten(),\n","    Dense(64, activation='relu'),\n","    Dropout(0.3),\n","    Dense(1, activation='sigmoid')  # Binary classification: cat or dog\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Print model summary\n","model.summary()\n"]},{"cell_type":"code","execution_count":24,"id":"7d133453","metadata":{"executionInfo":{"elapsed":427,"status":"ok","timestamp":1711877936916,"user":{"displayName":"Mahmoud Taha","userId":"01043581029630877833"},"user_tz":-120},"id":"7d133453"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 50ms/step - accuracy: 0.9955 - loss: 0.1907\n","Epoch 2/5\n","\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n","Epoch 3/5\n","\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n","Epoch 4/5\n","\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n","Epoch 5/5\n","\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n","<keras.src.callbacks.history.History object at 0x0000020A0C927110>\n"]}],"source":["\n","# Train the model\n","history = model.fit(np.array(x_train), np.array(y_train), epochs=5, batch_size=32)\n","print(history)\n"]},{"cell_type":"code","execution_count":25,"id":"e861f849","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":967,"status":"ok","timestamp":1711878033668,"user":{"displayName":"Mahmoud Taha","userId":"01043581029630877833"},"user_tz":-120},"id":"e861f849","outputId":"f5d0d267-29a9-490f-fad1-173451111228"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n","Test Loss: 0.0, Test Accuracy: 1.0\n"]}],"source":["# Evaluate model\n","loss, accuracy = model.evaluate(np.array(x_test), np.array(y_test))\n","print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"]},{"cell_type":"code","execution_count":28,"id":"UnjoG_f114n1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1417,"status":"ok","timestamp":1711878040208,"user":{"displayName":"Mahmoud Taha","userId":"01043581029630877833"},"user_tz":-120},"id":"UnjoG_f114n1","outputId":"df674576-b756-4fb8-f347-ff0c319a6a21"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n","Precision: 1.0\n","Recall: 1.0\n","Confusion Matrix:\n","[[5000]]\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\MAHMOUD TAHA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]}],"source":["from sklearn.metrics import confusion_matrix, precision_score, recall_score\n","\n","# Predict on test data\n","y_pred = model.predict(np.array(x_test))\n","\n","y_pred = (y_pred > 0.5).astype(int)\n","# Calculate precision and recall\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","\n","# Build confusion matrix\n","conf_matrix = confusion_matrix(y_test, y_pred)\n","print(\"Confusion Matrix:\")\n","print(conf_matrix)"]},{"cell_type":"code","execution_count":null,"id":"xXR6BRH95Psk","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"executionInfo":{"elapsed":488,"status":"ok","timestamp":1711878245773,"user":{"displayName":"Mahmoud Taha","userId":"01043581029630877833"},"user_tz":-120},"id":"xXR6BRH95Psk","outputId":"ee90008c-88f1-4a67-a07b-ae76bd8e1d63"},"outputs":[{"name":"stdout","output_type":"stream","text":["You must install pydot (`pip install pydot`) for `plot_model` to work.\n"]}],"source":["# pip install pydot\n","\n","from keras.utils import plot_model\n","\n","# Plot the model architecture\n","plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"]},{"cell_type":"code","execution_count":33,"id":"98ffb62c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"name":"stdout","output_type":"stream","text":["Model saved at: d:\\Artificial Intelligence\\Machine Learning\\_projects\\dogs_cats_classification\\_notebooks\\model_20240409_212734.h5\n"]}],"source":["## save the model with timestamp namd\n","\n","import os\n","from datetime import datetime\n","\n","# Get the current timestamp\n","timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","\n","# Get the current working directory of the notebook\n","cwd = os.getcwd()\n","\n","# Define the model name with the timestamp\n","model_name = f\"model_{timestamp}.h5\"\n","\n","# Save the model\n","model.save(os.path.join(cwd, model_name))\n","\n","# Print confirmation message\n","print(\"Model saved at:\", os.path.join(cwd, model_name))\n"]},{"cell_type":"code","execution_count":6,"id":"63812fdd","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n","Prediction: Dog\n"]}],"source":["## load the model and predict random sample\n","\n","import os\n","import cv2\n","import numpy as np\n","from tensorflow.keras.models import load_model\n","from tkinter import filedialog\n","import tkinter as tk\n","from datetime import datetime\n","\n","# Function to preprocess the image\n","def preprocess_image(image_path, target_size=(128, 128)):\n","    img = cv2.imread(image_path)\n","    img = cv2.resize(img, target_size)\n","    img = img / 255.0  # Normalize pixel values\n","    return img\n","\n","# Function to open file dialog and choose an image\n","def choose_image():\n","    root = tk.Tk()\n","    root.withdraw()  # Hide the root window\n","\n","    # Show file dialog to choose an image\n","    file_path = filedialog.askopenfilename(title=\"Choose an image\")\n","\n","    return file_path\n","\n","# Get the current working directory of the notebook\n","cwd = os.getcwd()\n","\n","# Get all .h5 model files in the current directory\n","model_files = [file for file in os.listdir(cwd) if file.endswith('.h5')]\n","\n","# Get the most recent model file\n","latest_model_file = max(model_files, key=os.path.getctime)\n","\n","# Load the most recent model\n","model = load_model(os.path.join(cwd, latest_model_file))\n","\n","# Call the function to choose an image\n","# img_path = choose_image()\n","img_path = \"D:\\\\Artificial Intelligence\\\\Machine Learning\\\\_projects\\\\dogs_cats_classification\\\\data\\\\random\\\\random_cat.jpg\"\n","if img_path:\n","    img = preprocess_image(img_path)\n","    img = np.expand_dims(img, axis=0)  # Add batch dimension\n","    prediction = model.predict(img)\n","    if prediction[0][0] > 0.5:\n","        print(\"Prediction: Dog\")\n","    else:\n","        print(\"Prediction: Cat\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":5}
